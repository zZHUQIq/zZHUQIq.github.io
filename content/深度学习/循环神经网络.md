1. 起源：
	1. Hopfield神经网络——具有递归特性，每一个神经元的输入来自于其他所有神经元，它的输出又传递给其他所有的神经元，但自身没有反馈连接。
	2. Jordan Network——Elman Network
	3. 基于时间的反向传播算法（Backward Propagation Through Time，BPTT）——实现了循环神经网络的有效训练
	4. LSTM：长短期记忆网络
	5. BRNN：双向循环神经网络
	6. GRU——简化LSTM结构
	7. 神经图灵机
	8. 堆叠神经网络：Stacked RNN
2. RNN—序列数据的提取和建模【文本+语言+视频+时态数据】
3. **循环神经网络定义**：循环神经网络是一种人工神经网络，它的节点间的连接形成一个**遵循时间序列**的有向图，即前部序列的信息处理后，作为输入信息传递到后部序列，但前部序列信息在传递到后部的同时，信息权重会下降，导致重要信息缺失，这会导致反向传播时梯度消失【***因此需要提高前部特定信息的决策权重***】![[Pasted image 20240511154540.png]]![[Pasted image 20240408085436.png]]***注***：两个输入+两个输出![[Pasted image 20240511154742.png]]***注***：h相当于隐藏层包含不同神经元，且后面的y都包含前面x的信息![[Pasted image 20240512115535.png]]![[Pasted image 20240512120435.png]]***注***：在识别文字任务中，字母字典需要构建【构建成数组定义字母的位置】
4. 训练算法：——**BPTT**【BP算法】![[Pasted image 20240511154904.png]]![[Pasted image 20240408090430.png]]***注***：S—损失函数的梯度；J—激活函数的梯度；X—线性变换![[Pasted image 20240511155236.png]]![[Pasted image 20240511155342.png]]

# 长短时记忆网络
1. LSTM【存在梯度消失问题，且不能解决长时依赖问题】：“门”机制对细胞状态信息进行添加或删除，且增加了**记忆细胞Ci**，实现对传递前部远处部位的重要信息【调制其激活函数，让其在传递过程中丢失的信息尽可能少】，由此实现长程记忆。![[Pasted image 20240512121919.png]]![[Pasted image 20240511160139.png]]![[Pasted image 20240512122229.png]]![[Pasted image 20240408091721.png]]![[Pasted image 20240408093036.png]]***注***：门通过权重【σ：激活函数】控制信息传递输入的程度![[Pasted image 20240408093431.png]]![[Pasted image 20240408093558.png]]***注***：i和f都是权重向量![[Pasted image 20240408094114.png]]![[Pasted image 20240511160501.png]]![[Pasted image 20240511160704.png]]![[Pasted image 20240512122346.png]]

# 循环神经网络变种
1. GRU：遗忘门和输入门用更新门来替代，GRU将细胞状态与隐状态合并![[Pasted image 20240408094830.png]]
2. 双向RNN(BRNN)：RNN(双向RNN)假设当前t的输出不仅仅和之前的序列有关，并且还与**之后的序列**【把后部的信息考虑进去：反向传播】有关，例如：完形填空【上下文的依赖】![[Pasted image 20240408095259.png]]
3. 堆叠RNN：X和Y之间多加了几个隐藏层，将多个RNN堆叠成多层RNN，每层RNN的输入为上一层RNN的输出。这种结构在某些NLP任务（如**机器翻译**）上有好的表现。
4. DRNN：深层循环神经网络——解决更复杂的序列任务![[Pasted image 20240512122909.png]]

# 循环神经网络的应用
1. ***神经网络语言模型***【词序列的预测，通过对语料库的统计学习，归纳出其中的语言知识，获得词与词之间的连接概率，并以词序列的概率为依据来判断其是否合理。】![[Pasted image 20240511161300.png]]
	1. 前馈神经网络![[Pasted image 20240511161707.png]]
	2. 循环神经网络![[Pasted image 20240511161803.png]]
	3. LSTM语言模型
	4. 基于注意力机制的语言模型：最先进
2. **自动文本摘要**【自动报告生成、新闻标题生成】：长文本的信息压缩——基于指针—生成器网络的生成式文本摘要模型![[Pasted image 20240511162042.png]]![[Pasted image 20240511162110.png]]
3. 机器阅读理解
	1. R-Net![[Pasted image 20240511162233.png]]
	2. BiDAF![[Pasted image 20240511162255.png]]
4. 情感识别：多输入单输出的RNN结构
5. 文章生成、音乐生成：单输入多输出RNN
6. 语言翻译：多输入多输出RNN