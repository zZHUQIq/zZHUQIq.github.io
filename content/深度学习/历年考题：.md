# 名词解释
1. 深度学习：![[Pasted image 20240512194123.png]]
2. 稀疏自编码器：![[Pasted image 20240512194302.png]]![[Pasted image 20240512194346.png]]
3. 正则化：![[Pasted image 20240512194629.png]]
4. 集成学习：![[Pasted image 20240512194725.png]]![[Pasted image 20240512194906.png]]
5. Dropout：![[Pasted image 20240512195032.png]]
6. 卷积神经网络：![[Pasted image 20240512195337.png]]
7. 循环神经网络：![[Pasted image 20240512195422.png]]
8. 奇异值分解：![[Pasted image 20240512195536.png]]![[Pasted image 20240512195550.png]]
9. 交叉熵/相对熵：![[Pasted image 20240512195707.png]]![[Pasted image 20240512195742.png]]![[Pasted image 20240512200104.png]]![[Pasted image 20240512214857.png]]
10. 深度信念网络：![[Pasted image 20240512195922.png]]
11. 欠拟合与过拟合：![[Pasted image 20240512200212.png]]
12. 深度森林：![[Pasted image 20240512200329.png]]
13. 降噪自编码器：![[Pasted image 20240512200418.png]]
14. 胶囊网络：![[Pasted image 20240512200553.png]]
15. 深度可分离卷积：![[Pasted image 20240512200648.png]]
16. 目标检测：![[Pasted image 20240512200738.png]]
17. 焦点损失：![[Pasted image 20240512200833.png]]![[Pasted image 20240512215203.png]]

# 简答题：
1. 请简述你对误差反向传播算法的理解：![[Pasted image 20240512201500.png]]![[Pasted image 20240512201535.png]]
2. 请列出卷积神经网络的主要结构模块，以及各个模块完成的功能——![[Pasted image 20240512201743.png]]
3. 请简述你对LSTM的理解，并解释为什么它能够解决长时依赖问题。![[Pasted image 20240512202042.png]]
4. 请简述深度学习中常见的避免过拟合的方法![[Pasted image 20240512202135.png]]![[Pasted image 20240512202346.png]]***注***：避免欠拟合的方法：![[Pasted image 20240512202549.png]]
5. 请简述你对生成对抗网络的理解，并简述其训练过程。![[Pasted image 20240512202941.png]]![[Pasted image 20240512203835.png]]![[Pasted image 20240512203853.png]]
6. 请简述你对胶囊网络的理解。![[Pasted image 20240512203042.png]]![[Pasted image 20240512203627.png]]
7. 请简述 Yolo 算法的主要思想和实现过程![[Pasted image 20240512203308.png]]![[Pasted image 20240512203339.png]]
8. 请简述GRU网络的主要思想，并用图和公式表达其计算过程![[Pasted image 20240512203441.png]]![[Pasted image 20240512203507.png]]
9. 请简述Dropout的实现方式，并阐述你理解的它对于解决过拟合问题的原因。![[Pasted image 20240512203956.png]]![[Pasted image 20240512204037.png]]
10. 请简述你对Batch Normalization的理解，并说明其在训练和测试阶段如何实现？![[Pasted image 20240512204247.png]]
11. 请简述你对残差网络的理解，并解释为什么它能够解决梯度消失问题。![[Pasted image 20240512204449.png]]![[Pasted image 20240512204501.png]]
12. 请写出对矩阵Amxn（m≠n）进行奇异值分解的过程。![[Pasted image 20240512204546.png]]![[Pasted image 20240512204628.png]]
13. 请简述随机梯度下降法的基本思想并图示说明![[Pasted image 20240512210213.png]]![[Pasted image 20240512210237.png]]![[Pasted image 20240512210259.png]]
14. 请简述Transformer的主要思想，并用图和公式说明。![[Pasted image 20240512210709.png]]
# 计算：
1. 多分类任务中，某个样本的期望输出为（0，0，0，1），两个模型A和B都采用交叉熵作为损失函数，针对该样本的实际输出分别为（In20，In40，In60，In80）、（In10，In30，ln50，In90），采用Softmax 函数对输出进行归一化并计算两个模型的交叉熵，说明哪个模型更好。提示：lg2≈0.301，lg3≈0.477。![[Pasted image 20240512213322.png]]
2. 计算焦点损失：![[Pasted image 20240513074706.png]]
3. 计算感受野：![[Pasted image 20240513074819.png]]![[Pasted image 20240513075002.png]]***注***：最后等于1就行
4. 计算参数量：卷积核大小 X 深度 X 通道数【特征图的通道数和前面所选卷积核深度相关】