# Motivating example
1. **回顾**：之前所有的state还有action是基于**表格**的形式——qπ(s,a)【编程的时候就把这些表格**存储成向量矩阵或者是数组**】![[Pasted image 20240527102700.png]]![[Pasted image 20240527102809.png]]***注***：但表格不能处理较大且连续数量的state！【缺乏一定的泛化能力——很多值其实可以通过泛化来减少计算量——**值函数近似**】
2. **值函数近似例子**：利用一条直线来拟合很多点【这相当于通过一条直线来拟合大量数据】![[Pasted image 20240527103118.png]]![[Pasted image 20240527103325.png]]***注***：优势在于不需要存储很多state value，只需要保存w两个参数；但近似不是特别精确![[Pasted image 20240527103508.png]]***注***：曲线拟合如下【**更复杂的曲线需要存储更多的值：但拟合的精度更高**】；v针对w任然是线性函数。![[Pasted image 20240527103539.png]]
3. **总结**：用函数来拟合vπ(s)——减少存储、泛化能力【只访问一些状态，相邻的状态的值也可以比较准确的被估计出来】![[Pasted image 20240527103953.png]]

# 估计状态价值的函数
1. **Objective function**【目标函数】：找最优w使得估计的v接近真实的v【policy evaluation】![[Pasted image 20240527104926.png]]![[Pasted image 20240527105145.png]]
	* 关于S的**分布**
		* 均匀分布【实际很难均匀：比如希望给主要的状态更大的权重】：![[Pasted image 20240527105315.png]]
		* **Stationary distribution**【long-run后趋向于平稳】：dπ(s)【也代表agent平稳的时候，访问某一状态的概率，如果s被访问的概率大，d就大】代表权重，权重越大，希望它与实际的误差更小![[Pasted image 20240527105628.png]]![[Pasted image 20240527105825.png]]![[Pasted image 20240527105948.png]]![[Pasted image 20240527110229.png]]![[Pasted image 20240527110315.png]]
2. **目标函数的优化**：求梯度——**随机梯度**![[Pasted image 20240527122700.png]]![[Pasted image 20240527122836.png]]***注***：vπ(St)未知——蒙特卡洛【当前状态得到的episode的一系列的reward——gt】/TD![[Pasted image 20240527123202.png]]![[Pasted image 20240527123244.png]]
3. Function approximators【V估计的选择】：![[Pasted image 20240527123630.png]]
	* 线性：TD-linear【需要选取很好的特征向量Φ】![[Pasted image 20240527123732.png]]![[Pasted image 20240527124103.png]]![[Pasted image 20240527124251.png]]
4. 例：**更多的参数得更好的拟合效果**【但参数个数很多那和tabular也没区别，也是一样的复杂，并且增加参数也不可能使得误差为0】![[Pasted image 20240527124431.png]]***注***：我们此时需要用**函数**得到一个和基于模型用贝尔曼算出真实state value所得到的类似的曲面——越接近越好！![[Pasted image 20240527124918.png]]![[Pasted image 20240527124949.png]]![[Pasted image 20240527125100.png]]![[Pasted image 20240527125158.png]]***注***：平面只能大致拟合一个趋势——因此需要用**复杂的高阶曲面**来拟合![[Pasted image 20240527125318.png]]![[Pasted image 20240527125403.png]]
5. **总结**：![[Pasted image 20240527125625.png]]
	*  数学上的不严谨：![[Pasted image 20240527125734.png]]![[Pasted image 20240527125841.png]]![[Pasted image 20240527125939.png]]![[Pasted image 20240527130046.png]]***注***：这里的w是一个投影矩阵。

# Sarsa 和Q-learning
1. **Sarsa算法的结合**：q替换了v![[Pasted image 20240527133834.png]]![[Pasted image 20240527134030.png]]
	* 例： ![[Pasted image 20240527134129.png]]
2. Q-learning的结合：需要进行梯度计算![[Pasted image 20240527134247.png]]![[Pasted image 20240527134318.png]]![[Pasted image 20240527134440.png]]
# Deep Q-learning
1. **定义**：深度神经网络【**非线性函数**】结合强化学习的算法![[Pasted image 20240527135000.png]]![[Pasted image 20240527141921.png]]
2. **优化**：![[Pasted image 20240527135142.png]]![[Pasted image 20240527135321.png]]***注***：使用上述两个网络的原因——先固定wT然后更新w，后面再把w【main】赋值给wT继续更新![[Pasted image 20240527135659.png]]![[Pasted image 20240527135757.png]]
3. **经验回放**：收集经验的时候有先后顺序，但使用的时候不一定需要按照先后顺序——从集合中随机取样(**均匀分布**：一个sample可以采样多次)来训练神经网络【经验回放】![[Pasted image 20240527140145.png]]![[Pasted image 20240527140354.png]]![[Pasted image 20240527140520.png]]***注***：这里解释为什么要用均匀分布【没有先验知识需要均匀分布】![[Pasted image 20240527141042.png]]
4. 例：**泛化能力+经验回放**——有效减小了训练次数![[Pasted image 20240527142118.png]]![[Pasted image 20240527142230.png]]
