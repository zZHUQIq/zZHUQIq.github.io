# Motivating examples
1. State value+action value——线性方程组![[Pasted image 20240521153354.png]]![[Pasted image 20240521153523.png]]
2. 当前策略不好时，如何提升?![[Pasted image 20240521153736.png]]![[Pasted image 20240521153829.png]]***注***：a* 代表action value最大的action!——不断迭代下去，即便一开始的策略很差，也会慢慢逼近最优策略![[Pasted image 20240521154148.png]]***注***：数学形式——贝尔曼最优公式

# 贝尔曼最优公式
1. **最优策略**：对于任意状态下采取的策略所产生的state value都要比其他的策略大，说明就是最优策略π* ![[Pasted image 20240521154431.png]]
2. **贝尔曼最优公式**：![[Pasted image 20240521154807.png]]![[Pasted image 20240521154950.png]]
3. 表达式的**分析思路**：![[Pasted image 20240521155308.png]]![[Pasted image 20240521155604.png]]![[Pasted image 20240521155643.png]]***注***：这相当于我给最大的那个action value赋予最大的权重，即为1

4. 贝尔曼公式的**重写**：需要固定v才能求解一个maxπ，一次max是一个关于v的函数![[Pasted image 20240521160307.png]]
5. **Contraction mapping theorem**：
	1. 不动点：f(x)是x的映射，若f(x)=x，代表我又映射到了我自己，这个点不再动，所以叫做不动点![[Pasted image 20240521160648.png]]
	2. Contraction mapping：![[Pasted image 20240521160917.png]]![[Pasted image 20240521161045.png]]![[Pasted image 20240521161255.png]]***注***：通过迭代，Xk会趋向于不动点
	3. 例：![[Pasted image 20240521161450.png]]![[Pasted image 20240521161526.png]]***注***：X是一个向量，A是一个矩阵
6. BOE的收缩性【收敛性】：![[Pasted image 20240521161636.png]]![[Pasted image 20240521161738.png]]![[Pasted image 20240521161916.png]]![[Pasted image 20240521162204.png]]***注***：value iteration algorithm【值迭代】
	* 例： ![[Pasted image 20240521162346.png]]![[Pasted image 20240521162539.png]]![[Pasted image 20240521162643.png]]***注***：贪婪策略选择最大的q-value![[Pasted image 20240521163023.png]]***注***：ε为很小的数
7.  贝尔曼最优公式解的**最优性**：贝尔曼最优公式是贝尔曼公式的特殊，所对应的策略为π*【确定的但不唯一；state value是唯一的】 ![[Pasted image 20240521163202.png]]![[Pasted image 20240521163308.png]]![[Pasted image 20240521163403.png]]***注***：π* 是确定的且贪婪的【选择p-value最大的】

# 最优策略的分析
1. 什么样的因素决定了最优策略？【红色已知，求解黑色】——由模型+r+γ决定【往往调整r和γ，模型不好调整】![[Pasted image 20240521192959.png]]
	1. **γ的调整**：![[Pasted image 20240521193234.png]]***注***：有时候最优策略不一定就完全绕开forbidden areas![[Pasted image 20240521193411.png]]***注***：**γ越大【远视】；γ较小【近视】**![[Pasted image 20240521193606.png]]***注***：γ=0，整个策略变得非常短视
	2. **r的调整**：如果惩罚较大，他会直接绕开forbidden area![[Pasted image 20240521193912.png]]![[Pasted image 20240521194036.png]]***注***：相对value不变就不会改变最终的最优策略【不看绝对值】![[Pasted image 20240521194329.png]]
	3. 例：γ的约束可以避免绕路![[Pasted image 20240521194540.png]]

# 总结![[Pasted image 20240521194732.png]]