# 马尔可夫决策
1. 确定的MDP：
	1. 确定状态转移的**MDP**：奖励函数可能是状态迁移函数也可能是它的一部分
	2. 确定MDP的优化目标：折扣因子【γ：符合利益追求】——代表当下性能指标的权重【越大程度越高—远视】，若系统变化快，折扣因子就会大些
2. 随机的MDP：当前状态采取的动作得到下一个状态是不确定的，存在一定的概率【P】
3. 函数
	1. 状态转移函数：P
	2. 状态价值函数(V)：某一状态或动作的长期价值——找最优决策进行收敛【vk最后约等于vk+1】
	3. 动作价值函数：q，长期和V没区别，受当前即时回报R和P、V的影响，体现出a的选择【值函数没有a的选择】
	4. 最优价值函数：V*(s)指在所有策略发生的状态价值函数中，使状态s价值最大的函数
4. 函数相关决定因素
	1. 回报(R)：某一也是存在时刻的奖励折扣的【R和P都受动作集a的影响】
	2. 策略(Π)：给定状态到动作的一种分布——也会影响P和R
	3. a:动作、s:状态
5. 最优策略：比较状态价值函数来进行评价策略的好坏——寻找最优策略可以通过最大化动作价值函数来寻找
	* 如何寻找最优？
		1. 动态规划，用规划进行预测和控制
		2. 迭代策略评估
	* 如何提升策略？选取贪心动作，使得vΠ最大【策略迭代：评估加更新】确定一个策略pi，然后使得V最大，通过最大的V对pi再进行调整
	* 价值迭代的收敛性
		* 算子T：算子作用于第k步，第k+1步的V不变
		* 无穷范数