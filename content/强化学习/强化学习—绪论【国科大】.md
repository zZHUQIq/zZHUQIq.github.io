***绪论***：
1. 效果定律：只有在实际执行了动作【如狗听到铃声分泌了唾液】并对结果【狗吃到了食物】进行了评估，学习才会产生。根据Thorndike的理论，响应(或动作)与执行情况(状态)联系的强度决定了动作的强度。——揭示了行为和其结果的关系【行为如果产生了令人满意的结果，那么这种行为在未来发生的可能性就会增加；如果产生了不满意或不愉快的结果，那么这种行为在未来发生的可能性就会减少。】
2. ![[Pasted image 20240530190619.png]]
# 强化学习定义
1. 强化学习基本原理【与控制理论的联系】：
	1. **马尔可夫理论**：存在无后效性(下一步的状态受上一步的状态【包含前面所有状态产生的结果】以及当前的action影响)，对时序决策进行建模——每一步结果所产生的评价累积产生长期性能指标——得到最优决策序列。*下围棋*
	2. **贝尔曼动态规划**：多级决策获取最优解——**最优性原理**【最优决策序列每一步作为初始状态都可以达到最优解】；But计算量会随着问题的维数成指数增长，导致维数灾难。*自动驾驶*
	3. **近似动态规划**：解决维数灾难【最优决策序列难以获得，用近似估计的方法估计特定初始状态到最优解的过程，从而进行不断迭代】
	4. **强化学习**——借鉴了动态规划和近似动态规划；策略迭代和值迭代发展了具有稳定性和收敛性的强化学习的算法；从而可以进行原理学习、长期规划
2. 成为强化学习的条件：
	1. **效果定律**【试错学习】：选择性(尝试不同的选择，通过比较后果进行选择)；关联性(选择与特定情境的关联，评估效果)
	2. 效果定律——搜索(尝试的过程)+记忆(把效果好的结合)
	3. 基于**试错学习**的强化学习；并且强化学习和监督学习不一样，其存在时序性、在线性以及探索性。 
# 动态规划
1. 原理：解决序列决策问题
2. 相关问题：
	1. 最短路线问题：从终点开始一步一步进行**逆向分级计算**，从后往前优化每一步，这符合最优性原理。
	2. 离散最优多级决策问题：也是逆向递推来寻找最有序列
	3. But**风险**是——决策越多会导致维数灾难，并且还包含不确定性【对可能需要求期望——采样求平均，需要大量的采样】，这个随机因素也会导致维数灾难——解决问题就用近似的方法来解决
3. 总结：动态规划——近似动态规划——强化学习