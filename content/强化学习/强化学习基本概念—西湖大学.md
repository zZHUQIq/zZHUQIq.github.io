1. **State**：Agent相对于环境的状态——如网格世界中的状态就是location![[Pasted image 20240520193600.png]]**State space**：状态空间，所有的状态的集合
3. **Action**：每一个状态都有可采取的行动![[Pasted image 20240520193732.png]]**Action space**：动作的集合，且不同状态所对应的动作不同，所以A是S的一个函数A(Si)
4. **State transition**：采取一个动作时，agent的状态发生转移【定义了agent与环境发生交互的行为】![[Pasted image 20240520194047.png]]![[Pasted image 20240520194418.png]]***注***：表格只能表述行动确定性的情况，随机性的例子需要**State transition probability**【定义了在当前s状态下，采取动作a，跳到了一个状态s的概率】![[Pasted image 20240520194919.png]]
5. **Policy**：策略π——告诉agent在当前状态下应该怎么走【在当前状态下采取动作a的概率，因此对应一个状态采取的所有动作的概率之和为1】，基于策略可以得到路径![[Pasted image 20240520195118.png]]![[Pasted image 20240520195253.png]]***注***：此时策略是确定的，s1状态就只会采取a1的动作
6. **Reward**：r为正的代表对行为的鼓励，负的代表对行为的惩罚![[Pasted image 20240520213633.png]]![[Pasted image 20240520213751.png]]***注***：reward是引导agent该怎么做，从而实现目标![[Pasted image 20240520214019.png]]***注***：reward依赖于当前的状态和Action而不是依赖于下一个状态【相同的下一个状态所对应的reward可能不同】
7. **Trajectory**：State-action-reward链![[Pasted image 20240520214355.png]]**Return**：把沿着轨迹的所有reward加起来【Return the greater，policy越好】，可以用于评估一个策略是好还是坏
8. **Discounted return**：轨迹是无限长的，因此加上**折扣因子γ**可以有效避免return无限大![[Pasted image 20240520214800.png]]![[Pasted image 20240520214845.png]]***注***：**discount rate**—γ；γ接近0，那么return会依赖最开始的reward【注重当下】，反之γ趋向1，未来的reward衰减慢，积累起来的return也多依赖未来的reward【考虑未来】
9. **episode**：无**terminal state**的任务叫做continuing tasks【任务会一直持续下去】![[Pasted image 20240520215427.png]]![[Pasted image 20240520215620.png]]
10. ***马尔科夫链MDP***：**memoryless property**【马尔科夫性：不考虑历史，历史无关性】![[Pasted image 20240520220329.png]]***注***：Markov——property；decision——policy；process——Sets+Probability![[Pasted image 20240520220720.png]]***注***：decision是确定的，MDP——MP【马尔科夫过程】
11. 总结：![[Pasted image 20240520220831.png]]