# Policy gradient
1. 查表法【策略】：![[Pasted image 20240527143527.png]]
2. **表格——函数**【参数：θ】：泛化能力的增强![[Pasted image 20240527143809.png]]![[Pasted image 20240527143914.png]]***注***：函数法需要我们定义一个标量的目标函数，然后去优化这个目标函数![[Pasted image 20240527144113.png]]***注***：概率对于函数需要计算![[Pasted image 20240527144206.png]]***注***：需要改变θ才能更新策略
3. **Policy gradient基本思路**：![[Pasted image 20240527144419.png]]

# Metrics to define optimal policies
1. **Average value**：策略的函数![[Pasted image 20240527144837.png]]![[Pasted image 20240527144927.png]]
	1. d和策略独立：![[Pasted image 20240527145231.png]]![[Pasted image 20240527145335.png]]
	2. d和策略非独立：根据策略——访问多的权重大，访问少的权重小![[Pasted image 20240527145545.png]]
2. **Average reward**：策略的函数【immediate r】![[Pasted image 20240527145937.png]]![[Pasted image 20240527151319.png]]***注***：s0可以省略，因为最后s0不起作用![[Pasted image 20240527151602.png]]
3. Metric补充：
	1. 上述的V以及R都是θ的函数，因此需要更新θ来最优化这两个metrics
	2. ![[Pasted image 20240527152012.png]]
	3. 二者关系：![[Pasted image 20240527152057.png]]
	4. 拓展：J(θ)=v(π)平均![[Pasted image 20240527152345.png]]![[Pasted image 20240527152549.png]]

# Gradients of the metrics
1. **梯度定义**：![[Pasted image 20240527195535.png]]![[Pasted image 20240527195711.png]]![[Pasted image 20240527195757.png]]***注***：通过采样来近似梯度![[Pasted image 20240527200127.png]]![[Pasted image 20240527200301.png]]
2. 归一化处理：softmax函数——策略是探索性的![[Pasted image 20240527200648.png]]***注***：h(s,a,θ)是一个特征函数![[Pasted image 20240527200912.png]]![[Pasted image 20240527200944.png]]***注***：若action为无穷多个就不适用

# Reinforce
1. **梯度上升**：随机梯度替代真实梯度，但qπ(st,at)也是**未知的**——蒙特卡洛![[Pasted image 20240527201158.png]]![[Pasted image 20240527201340.png]]
	1. 如何采样？![[Pasted image 20240527201658.png]]
	2. 梯度上升：通过改变θ来优化π(at|st,θ)![[Pasted image 20240527201832.png]]![[Pasted image 20240527202225.png]]![[Pasted image 20240527202530.png]]***注***：探索和充分利用的平衡——βt

2. Reinforce：基于蒙特卡洛进行策略更新![[Pasted image 20240527202706.png]]![[Pasted image 20240527202825.png]] ^c86883
3. Summary